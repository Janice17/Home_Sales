# Home_Sales
Using knowledge of SparkSQL to determine key metrics about home sales data. Then using Spark to create temporary views, partition the data, cache and uncache a temporary table, and verify that the table has been unchached.
# Code Source:
Jupyter Notebook code (Home_Sales_colab.ipynb)
# Key Metrics about Home Sales data:
1. Renamed file to Home_Sales_colab.ipynb.  
2. Imported the necessary PySpark SQL functions.  
3. Read the home_sales_revised.csv from the provied AWS S3 bucket location into a PySpark DataFrame.  
4. Created a temporary table called home_sales.  
5. Answered the following questions using SparkSQL:  
- What is the average price for a four-bedroom house sold for each year? Answered within Home_Sales_colab.ipynb notebook.  
- What is the average price of a home for each year the home was built, that has three bedrooms and three bathrooms? Answered within Home_Sales_colab.ipynb notebook.  
- What is the average price of a home for each year the home was built, that has three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet? Answered within Home_Sales_colab.ipynb notebook.  
- What is the average price of a home per "view" rating having an average home price greater than or equal to $350,000 (Determine the run time for this query)? Answered within Home_Sales_colab.ipynb notebook (1.83 seconds).  
6. Cache temporary table home_sales.  
7. Checked if temporary table is cached.  
8. Used the cached data, ran the last query that calculates the average price of a home per "view" rating having average home price greater than or equal to $350,000. The run time for this query is less than uncached run time (1.83 > 0.50).   
9. Partition by the "date_built" field on the formatted parquet home sales data.  
10. Created a temporary table for the parquet data.  
11. Ran the last query that calculates the average price of a home per "view" rating having an average home price greater than or equal to $350,000. The runtime compared to the uncached runtime is still less (1.83 > 0.68).  
12. Uncache the home_sales temporary tables.  
13. Verified that the home_sales temporary table is uncached using PySpark.  
14. Download and upload "Home_Sales" GitHub repository.
# References:
Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.